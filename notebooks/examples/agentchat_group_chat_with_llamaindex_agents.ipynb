{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGen Group Chat with LlamaIndex Agents\n",
    "\n",
    "This notebook demonstrates how to integrate AutoGen's group chat functionality with LlamaIndex agents to create a powerful multi-agent system.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This example shows:\n",
    "- Setting up AutoGen agents\n",
    "- Integrating LlamaIndex for document retrieval\n",
    "- Creating a group chat with multiple specialized agents\n",
    "- Coordinating agents to solve complex tasks\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Install required packages:\n",
    "```bash\n",
    "pip install pyautogen llama-index openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import autogen\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.llms.openai import OpenAI as LlamaOpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up your OpenAI API key and configure the LLM settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Configure AutoGen\n",
    "config_list = [\n",
    "    {\n",
    "        \"model\": \"gpt-4\",\n",
    "        \"api_key\": os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    }\n",
    "]\n",
    "\n",
    "# LLM configuration\n",
    "llm_config = {\n",
    "    \"timeout\": 600,\n",
    "    \"cache_seed\": 42,\n",
    "    \"config_list\": config_list,\n",
    "    \"temperature\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up LlamaIndex\n",
    "\n",
    "Create a LlamaIndex instance for document retrieval and question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LlamaIndex settings\n",
    "Settings.llm = LlamaOpenAI(model=\"gpt-4\", temperature=0)\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Load documents (replace with your document directory)\n",
    "# documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "# index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# For demo purposes, create a simple index\n",
    "sample_documents = [\n",
    "    \"SurfSense is an AI research assistant that integrates with your personal knowledge base.\",\n",
    "    \"It supports multiple file formats including PDF, DOCX, images, and videos.\",\n",
    "    \"SurfSense uses advanced RAG techniques with hybrid search and rerankers.\",\n",
    "    \"You can generate podcasts from your chat conversations or documents.\",\n",
    "]\n",
    "\n",
    "# Create query engine\n",
    "# query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agents\n",
    "\n",
    "Create specialized agents for different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. User Proxy Agent - represents the human user\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_Proxy\",\n",
    "    system_message=\"A human user who provides tasks and feedback.\",\n",
    "    code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n",
    "    human_input_mode=\"NEVER\",  # Change to \"ALWAYS\" for interactive mode\n",
    ")\n",
    "\n",
    "# 2. Document Researcher - uses LlamaIndex for document retrieval\n",
    "document_researcher = autogen.AssistantAgent(\n",
    "    name=\"Document_Researcher\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"You are a document researcher. \n",
    "    Your role is to search through documents and provide relevant information.\n",
    "    Use the available tools to query the document index and provide accurate, cited answers.\n",
    "    Always cite your sources when providing information from documents.\"\"\",\n",
    ")\n",
    "\n",
    "# 3. Data Analyst - analyzes information and provides insights\n",
    "data_analyst = autogen.AssistantAgent(\n",
    "    name=\"Data_Analyst\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"You are a data analyst.\n",
    "    Your role is to analyze information, identify patterns, and provide insights.\n",
    "    You work with data provided by other agents and synthesize meaningful conclusions.\"\"\",\n",
    ")\n",
    "\n",
    "# 4. Report Writer - creates comprehensive reports\n",
    "report_writer = autogen.AssistantAgent(\n",
    "    name=\"Report_Writer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"You are a professional report writer.\n",
    "    Your role is to compile information from other agents into clear, well-structured reports.\n",
    "    Create comprehensive summaries with proper formatting and citations.\"\"\",\n",
    ")\n",
    "\n",
    "# 5. Code Executor - runs code and validates results\n",
    "code_executor = autogen.AssistantAgent(\n",
    "    name=\"Code_Executor\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"You are a code execution specialist.\n",
    "    Your role is to write and execute Python code to process data, create visualizations, or perform calculations.\n",
    "    Always provide clear explanations of your code and results.\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LlamaIndex Query Function\n",
    "\n",
    "Define a function that allows agents to query the LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_documents(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Query the document index using LlamaIndex.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        \n",
    "    Returns:\n",
    "        The response from the query engine\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # In a real implementation, you would use:\n",
    "        # response = query_engine.query(query)\n",
    "        # return str(response)\n",
    "        \n",
    "        # Demo response\n",
    "        return f\"\"\"Based on the documents, here's what I found about '{query}':\n",
    "        SurfSense is a comprehensive AI research assistant that integrates with your \n",
    "        personal knowledge base. It supports 50+ file formats and uses advanced RAG \n",
    "        techniques including hybrid search, vector embeddings, and rerankers for \n",
    "        optimal information retrieval.\"\"\"\n",
    "    except Exception as e:\n",
    "        return f\"Error querying documents: {str(e)}\"\n",
    "\n",
    "# Register the function with the document researcher agent\n",
    "autogen.register_function(\n",
    "    query_documents,\n",
    "    caller=document_researcher,\n",
    "    executor=user_proxy,\n",
    "    name=\"query_documents\",\n",
    "    description=\"Query the document index to retrieve relevant information. Use this when you need to find specific information from documents.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Group Chat\n",
    "\n",
    "Create a group chat with all agents and define the conversation flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create group chat\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, document_researcher, data_analyst, report_writer, code_executor],\n",
    "    messages=[],\n",
    "    max_round=12,\n",
    "    speaker_selection_method=\"round_robin\",  # or \"auto\" for automatic selection\n",
    ")\n",
    "\n",
    "# Create manager\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat,\n",
    "    llm_config=llm_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Simple Document Query Task\n",
    "\n",
    "Ask the agents to research and summarize information about SurfSense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the conversation\n",
    "task_1 = \"\"\"\n",
    "Research SurfSense and create a brief summary report that includes:\n",
    "1. What is SurfSense?\n",
    "2. What are its key features?\n",
    "3. What technologies does it use?\n",
    "\n",
    "Document Researcher: Query the documents for information.\n",
    "Data Analyst: Analyze the information.\n",
    "Report Writer: Create a final summary report.\n",
    "\"\"\"\n",
    "\n",
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=task_1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Complex Multi-Step Task\n",
    "\n",
    "A more complex task involving research, data analysis, and code execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_2 = \"\"\"\n",
    "Perform a comprehensive analysis of AI agent frameworks:\n",
    "\n",
    "1. Document Researcher: Find information about CrewAI, AutoGen, and LangGraph\n",
    "2. Data Analyst: Compare their features and use cases\n",
    "3. Code Executor: Create a simple visualization comparing the frameworks\n",
    "4. Report Writer: Compile everything into a final report with recommendations\n",
    "\n",
    "Please work together to complete this task.\n",
    "\"\"\"\n",
    "\n",
    "# Reset the group chat for a fresh conversation\n",
    "groupchat.messages = []\n",
    "\n",
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=task_2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Custom Speaker Selection\n",
    "\n",
    "Implement a custom speaker selection method for more control over the conversation flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_speaker_selection_func(last_speaker, groupchat):\n",
    "    \"\"\"\n",
    "    Custom function to determine the next speaker based on the conversation context.\n",
    "    \n",
    "    Args:\n",
    "        last_speaker: The agent who spoke last\n",
    "        groupchat: The group chat object\n",
    "        \n",
    "    Returns:\n",
    "        The next agent to speak\n",
    "    \"\"\"\n",
    "    messages = groupchat.messages\n",
    "    \n",
    "    # If no messages yet, start with user proxy\n",
    "    if len(messages) == 0:\n",
    "        return user_proxy\n",
    "    \n",
    "    # Get the last message\n",
    "    last_message = messages[-1][\"content\"].lower()\n",
    "    \n",
    "    # Decision logic based on keywords in the last message\n",
    "    if \"query\" in last_message or \"search\" in last_message or \"document\" in last_message:\n",
    "        return document_researcher\n",
    "    elif \"analyze\" in last_message or \"compare\" in last_message:\n",
    "        return data_analyst\n",
    "    elif \"code\" in last_message or \"visualize\" in last_message or \"plot\" in last_message:\n",
    "        return code_executor\n",
    "    elif \"report\" in last_message or \"summary\" in last_message or \"conclude\" in last_message:\n",
    "        return report_writer\n",
    "    else:\n",
    "        # Default to round-robin if no keywords match\n",
    "        agents = groupchat.agents\n",
    "        last_idx = agents.index(last_speaker)\n",
    "        return agents[(last_idx + 1) % len(agents)]\n",
    "\n",
    "# Create group chat with custom speaker selection\n",
    "groupchat_custom = autogen.GroupChat(\n",
    "    agents=[user_proxy, document_researcher, data_analyst, report_writer, code_executor],\n",
    "    messages=[],\n",
    "    max_round=12,\n",
    "    speaker_selection_method=custom_speaker_selection_func,\n",
    ")\n",
    "\n",
    "manager_custom = autogen.GroupChatManager(\n",
    "    groupchat=groupchat_custom,\n",
    "    llm_config=llm_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with SurfSense\n",
    "\n",
    "Example of how this could integrate with SurfSense's backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo-code for SurfSense integration\n",
    "\n",
    "class SurfSenseAgentChat:\n",
    "    \"\"\"\n",
    "    Integration wrapper for using AutoGen + LlamaIndex in SurfSense.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, search_space_id: int, user_id: int):\n",
    "        self.search_space_id = search_space_id\n",
    "        self.user_id = user_id\n",
    "        # Initialize agents and index\n",
    "        self.setup_agents()\n",
    "        \n",
    "    def setup_agents(self):\n",
    "        \"\"\"Set up agents with SurfSense-specific configurations.\"\"\"\n",
    "        # Load user's documents from SurfSense database\n",
    "        # Create LlamaIndex from user's search space\n",
    "        # Initialize AutoGen agents\n",
    "        pass\n",
    "    \n",
    "    def query_with_agents(self, user_query: str) -> dict:\n",
    "        \"\"\"\n",
    "        Process a user query using the agent group chat.\n",
    "        \n",
    "        Args:\n",
    "            user_query: The user's question or task\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing the conversation history and final result\n",
    "        \"\"\"\n",
    "        # Initiate group chat\n",
    "        # Collect results\n",
    "        # Return structured response\n",
    "        pass\n",
    "\n",
    "# Example usage:\n",
    "# agent_chat = SurfSenseAgentChat(search_space_id=1, user_id=123)\n",
    "# result = agent_chat.query_with_agents(\"Analyze my recent research papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "1. **Agent Specialization**: Create agents with clear, specific roles\n",
    "2. **Task Decomposition**: Break complex tasks into smaller subtasks for each agent\n",
    "3. **Error Handling**: Implement robust error handling in custom functions\n",
    "4. **Token Management**: Monitor token usage to stay within API limits\n",
    "5. **Conversation History**: Keep track of conversation context for better results\n",
    "6. **Testing**: Test your agent system with various scenarios before production use\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Experiment with different agent configurations\n",
    "- Add more specialized agents (e.g., fact checker, quality assurance)\n",
    "- Integrate with real document collections\n",
    "- Implement advanced speaker selection logic\n",
    "- Add monitoring and logging for production use\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [AutoGen Documentation](https://microsoft.github.io/autogen/stable/)\n",
    "- [LlamaIndex Documentation](https://docs.llamaindex.ai/)\n",
    "- [SurfSense Documentation](https://www.surfsense.net/docs/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
