# WebMagic ç½‘ç»œçˆ¬è™«é›†æˆæŒ‡å— | WebMagic Web Crawler Integration Guide

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨ WebMagic ä½œä¸º SurfSense çš„ç½‘ç»œçˆ¬è™«æ›¿ä»£æ–¹æ¡ˆã€‚

This guide explains how to use WebMagic as an alternative web crawler for SurfSense.

---

## ğŸ“‹ å…³äº WebMagic | About WebMagic

WebMagic æ˜¯ä¸€ä¸ªç®€å•çµæ´»çš„ Java çˆ¬è™«æ¡†æ¶ï¼Œå®Œå…¨æ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒå¤šçº¿ç¨‹æŠ“å–ã€‚å®ƒå€Ÿé‰´äº† Scrapy çš„è®¾è®¡æ€æƒ³ï¼Œæ˜¯ä¸€ä¸ªä¼˜ç§€çš„å›½äº§å¼€æºçˆ¬è™«æ¡†æ¶ã€‚

WebMagic is a simple and flexible Java web crawler framework with a fully modular design supporting multi-threaded crawling. Inspired by Scrapy's design philosophy, it's an excellent open-source Chinese crawler framework.

### æ ¸å¿ƒç‰¹æ€§ | Key Features

- âœ… **ç®€å•æ˜“ç”¨** - å®Œå…¨æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•
- âœ… **é«˜æ€§èƒ½** - æ”¯æŒå¤šçº¿ç¨‹æŠ“å–ï¼Œæ€§èƒ½ä¼˜å¼‚
- âœ… **çµæ´»é…ç½®** - æ”¯æŒæ³¨è§£å’Œé“¾å¼ API ä¸¤ç§é…ç½®æ–¹å¼
- âœ… **åˆ†å¸ƒå¼æ”¯æŒ** - æ”¯æŒåˆ†å¸ƒå¼çˆ¬è™«
- âœ… **ä¸°å¯Œçš„ç»„ä»¶** - å†…ç½®å¤šç§ä¸‹è½½å™¨ã€å¤„ç†å™¨å’ŒæŒä¹…åŒ–æ–¹æ¡ˆ
- âœ… **ä¸­æ–‡å‹å¥½** - å®Œå–„çš„ä¸­æ–‡æ–‡æ¡£å’Œç¤¾åŒºæ”¯æŒ

---

## ğŸ”— é¡¹ç›®ä¿¡æ¯ | Project Information

- **GitHub ä»“åº“**: [https://github.com/code4craft/webmagic](https://github.com/code4craft/webmagic)
- **Gitee é•œåƒ**: [https://gitee.com/flashsword20/webmagic.git](https://gitee.com/flashsword20/webmagic.git)
- **å®˜æ–¹æ–‡æ¡£**: [http://webmagic.io/docs/](http://webmagic.io/docs/)
- **å¼€å‘è¯­è¨€**: Java
- **License**: Apache License 2.0

---

## ğŸ¯ ä¸ºä»€ä¹ˆé€‰æ‹© WebMagic? | Why Choose WebMagic?

### SurfSense å½“å‰çˆ¬è™«æ–¹æ¡ˆ | SurfSense Current Crawler

SurfSense ç›®å‰ä½¿ç”¨ï¼š
- **Firecrawl** - åŸºäºäº‘çš„ç½‘é¡µæŠ“å–æœåŠ¡
- **AsyncChromiumLoader** - åŸºäº Chromium çš„å¼‚æ­¥åŠ è½½å™¨

### WebMagic çš„ä¼˜åŠ¿ | WebMagic Advantages

| ç‰¹æ€§ | WebMagic | Firecrawl | AsyncChromiumLoader |
|-----|----------|-----------|---------------------|
| **éƒ¨ç½²æ–¹å¼** | è‡ªæ‰˜ç®¡ Java åº”ç”¨ | äº‘æœåŠ¡/è‡ªæ‰˜ç®¡ | éœ€è¦æµè§ˆå™¨ä¾èµ– |
| **æˆæœ¬** | å…è´¹å¼€æº | æŒ‰é‡ä»˜è´¹/è‡ªæ‰˜ç®¡ | å…è´¹ |
| **æ€§èƒ½** | é«˜æ€§èƒ½å¤šçº¿ç¨‹ | ä¾èµ–æœåŠ¡ | è¾ƒé‡ |
| **JavaScript æ¸²æŸ“** | éœ€é…ç½® Selenium | æ”¯æŒ | åŸç”Ÿæ”¯æŒ |
| **ä¸­æ–‡æ”¯æŒ** | ä¼˜ç§€ | ä¸€èˆ¬ | ä¸€èˆ¬ |
| **åˆ†å¸ƒå¼** | åŸç”Ÿæ”¯æŒ | æœ‰é™ | ä¸æ”¯æŒ |
| **å®šåˆ¶åŒ–** | é«˜åº¦çµæ´» | æœ‰é™ | ä¸­ç­‰ |

### é€‚ç”¨åœºæ™¯ | Use Cases

WebMagic ç‰¹åˆ«é€‚åˆä»¥ä¸‹åœºæ™¯ï¼š

1. **å¤§è§„æ¨¡æ•°æ®æŠ“å–** - éœ€è¦çˆ¬å–å¤§é‡ç½‘é¡µæ—¶
2. **ä¸­æ–‡ç½‘ç«™çˆ¬å–** - é’ˆå¯¹ä¸­æ–‡ç½‘ç«™ä¼˜åŒ–
3. **å®šåˆ¶åŒ–éœ€æ±‚** - éœ€è¦é«˜åº¦å®šåˆ¶åŒ–çš„çˆ¬è™«é€»è¾‘
4. **æˆæœ¬æ•æ„Ÿ** - å¸Œæœ›é¿å…äº‘æœåŠ¡è´¹ç”¨
5. **å†…ç½‘éƒ¨ç½²** - éœ€è¦åœ¨å†…ç½‘ç¯å¢ƒä½¿ç”¨

---

## ğŸš€ å¿«é€Ÿå¼€å§‹ | Quick Start

### 1. ç¯å¢ƒè¦æ±‚ | Requirements

- **Java**: JDK 8 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼ˆæ¨è JDK 11 æˆ– JDK 17ï¼‰
- **Maven**: 3.x æˆ–æ›´é«˜ç‰ˆæœ¬ï¼ˆæ¨èï¼‰

### 2. Maven ä¾èµ– | Maven Dependency

```xml
<dependency>
    <groupId>us.codecraft</groupId>
    <artifactId>webmagic-core</artifactId>
    <version>0.9.1</version>
</dependency>
<dependency>
    <groupId>us.codecraft</groupId>
    <artifactId>webmagic-extension</artifactId>
    <version>0.9.1</version>
</dependency>
```

### 3. ç®€å•ç¤ºä¾‹ | Basic Example

```java
import us.codecraft.webmagic.Page;
import us.codecraft.webmagic.Site;
import us.codecraft.webmagic.Spider;
import us.codecraft.webmagic.processor.PageProcessor;

public class SurfSenseCrawler implements PageProcessor {
    
    private Site site = Site.me()
            .setRetryTimes(3)
            .setSleepTime(1000)
            .setCharset("UTF-8");

    @Override
    public void process(Page page) {
        // æå–æ ‡é¢˜
        String title = page.getHtml()
                .xpath("//title/text()").toString();
        
        // æå–æ­£æ–‡å†…å®¹
        String content = page.getHtml()
                .xpath("//body/text()").toString();
        
        // æå–æ‰€æœ‰é“¾æ¥
        page.addTargetRequests(
            page.getHtml().links().regex(".*").all()
        );
        
        // å­˜å‚¨ç»“æœ
        page.putField("title", title);
        page.putField("content", content);
        page.putField("url", page.getUrl().toString());
    }

    @Override
    public Site getSite() {
        return site;
    }

    public static void main(String[] args) {
        Spider.create(new SurfSenseCrawler())
                .addUrl("https://example.com")
                .thread(5)
                .run();
    }
}
```

---

## ğŸ”§ ä¸ SurfSense é›†æˆ | Integration with SurfSense

### é›†æˆæ–¹æ¡ˆ | Integration Approach

æœ‰ä¸¤ç§æ–¹å¼å°† WebMagic ä¸ SurfSense é›†æˆï¼š

#### æ–¹æ¡ˆ A: ç‹¬ç«‹çˆ¬è™«æœåŠ¡ | Standalone Crawler Service

åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„ Java å¾®æœåŠ¡ï¼Œé€šè¿‡ REST API ä¸ SurfSense åç«¯é€šä¿¡ã€‚

**æ¶æ„æµç¨‹**:
1. SurfSense åç«¯å‘é€çˆ¬å–è¯·æ±‚åˆ° WebMagic æœåŠ¡
2. WebMagic æœåŠ¡æ‰§è¡Œçˆ¬å–ä»»åŠ¡
3. çˆ¬å–ç»“æœé€šè¿‡ API è¿”å›ç»™ SurfSense
4. SurfSense å¤„ç†å¹¶å­˜å‚¨åˆ°æ•°æ®åº“

**ä¼˜åŠ¿**:
- æŠ€æœ¯æ ˆç‹¬ç«‹ï¼Œæ˜“äºç»´æŠ¤
- å¯ä»¥ç‹¬ç«‹æ‰©å±•çˆ¬è™«æœåŠ¡
- ä¸å½±å“ç°æœ‰æ¶æ„

#### æ–¹æ¡ˆ B: Python-Java æ¡¥æ¥ | Python-Java Bridge

ä½¿ç”¨ `JPype` æˆ– `Py4J` åœ¨ Python ä»£ç ä¸­è°ƒç”¨ WebMagicã€‚

**ä¼˜åŠ¿**:
- é›†æˆæ›´ç´§å¯†
- å‡å°‘ç½‘ç»œè°ƒç”¨å¼€é”€
- ç»Ÿä¸€éƒ¨ç½²

**åŠ£åŠ¿**:
- éœ€è¦ JVM ç¯å¢ƒ
- é…ç½®è¾ƒå¤æ‚

### æ¨èå®ç° | Recommended Implementation

**æ–¹æ¡ˆ A (ç‹¬ç«‹æœåŠ¡)** æ˜¯æ¨èæ–¹æ¡ˆï¼Œå› ä¸ºï¼š
- ä¿æŒ SurfSense æ¶æ„ç®€æ´
- WebMagic æœåŠ¡å¯é€‰ï¼Œä¸å½±å“ç°æœ‰åŠŸèƒ½
- æ˜“äºä¸ºä¸åŒç”¨æˆ·æä¾›å®šåˆ¶åŒ–çˆ¬è™«æœåŠ¡

---

## ğŸ“ REST API æœåŠ¡ç¤ºä¾‹ | REST API Service Example

### Spring Boot WebMagic æœåŠ¡ | Spring Boot WebMagic Service

```java
// Required imports
import org.springframework.web.bind.annotation.*;
import us.codecraft.webmagic.Page;
import us.codecraft.webmagic.Site;
import us.codecraft.webmagic.Spider;
import us.codecraft.webmagic.Task;
import us.codecraft.webmagic.ResultItems;
import us.codecraft.webmagic.pipeline.Pipeline;
import us.codecraft.webmagic.processor.PageProcessor;

import java.util.*;

// Request and Response DTOs
public class CrawlRequest {
    private String url;
    private Integer threads;
    private Integer depth;
    
    // Getters and setters
    public String getUrl() { return url; }
    public void setUrl(String url) { this.url = url; }
    public Integer getThreads() { return threads; }
    public void setThreads(Integer threads) { this.threads = threads; }
    public Integer getDepth() { return depth; }
    public void setDepth(Integer depth) { this.depth = depth; }
}

public class CrawlResult {
    private String url;
    private String status;
    private Map<String, Object> data;
    
    public CrawlResult(String url, String status, Map<String, Object> data) {
        this.url = url;
        this.status = status;
        this.data = data;
    }
    
    // Getters and setters
    public String getUrl() { return url; }
    public void setUrl(String url) { this.url = url; }
    public String getStatus() { return status; }
    public void setStatus(String status) { this.status = status; }
    public Map<String, Object> getData() { return data; }
    public void setData(Map<String, Object> data) { this.data = data; }
}

// Custom PageProcessor implementation
class CustomPageProcessor implements PageProcessor {
    private CrawlRequest request;
    private Site site;
    
    public CustomPageProcessor(CrawlRequest request) {
        this.request = request;
        this.site = Site.me()
            .setRetryTimes(3)
            .setSleepTime(1000)
            .setCharset("UTF-8");
    }
    
    @Override
    public void process(Page page) {
        // Extract data based on request configuration
        page.putField("title", page.getHtml().xpath("//title/text()").toString());
        page.putField("content", page.getHtml().xpath("//body/text()").toString());
        page.putField("url", page.getUrl().toString());
    }
    
    @Override
    public Site getSite() {
        return site;
    }
}

// Controller implementation
@RestController
@RequestMapping("/api/crawler")
public class WebMagicController {
    
    @PostMapping("/crawl")
    public CrawlResult crawlUrl(@RequestBody CrawlRequest request) {
        // Create result items collector
        ResultItemsCollectorPipeline collector = new ResultItemsCollectorPipeline();
        
        // Create spider instance
        CustomPageProcessor processor = new CustomPageProcessor(request);
        Spider spider = Spider.create(processor)
                .addUrl(request.getUrl())
                .addPipeline(collector)
                .thread(request.getThreads() != null ? request.getThreads() : 5);
        
        // Execute crawl
        spider.run();
        
        // Extract collected data
        Map<String, Object> extractedData = new HashMap<>();
        extractedData.put("pageCount", collector.getResults().size());
        extractedData.put("items", collector.getResults());
        
        // Return result
        return new CrawlResult(
            request.getUrl(),
            "completed",
            extractedData
        );
    }
}

// Helper class to collect results
class ResultItemsCollectorPipeline implements Pipeline {
    private List<ResultItems> results = new ArrayList<>();
    
    @Override
    public void process(ResultItems resultItems, Task task) {
        if (!resultItems.isSkip()) {
            results.add(resultItems);
        }
    }
    
    public List<ResultItems> getResults() {
        return results;
    }
}
```

### SurfSense åç«¯è°ƒç”¨ç¤ºä¾‹ | SurfSense Backend Call Example

```python
import httpx
from typing import Optional

class WebMagicClient:
    """WebMagic crawler service client."""
    
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.client = httpx.AsyncClient(timeout=30.0)
    
    async def crawl_url(
        self,
        url: str,
        threads: int = 5,
        depth: int = 1
    ) -> dict:
        """Send crawl request to WebMagic service."""
        response = await self.client.post(
            f"{self.base_url}/api/crawler/crawl",
            json={
                "url": url,
                "threads": threads,
                "depth": depth
            }
        )
        response.raise_for_status()
        return response.json()
    
    async def get_status(self, job_id: str) -> dict:
        """Get crawl job status."""
        response = await self.client.get(
            f"{self.base_url}/api/crawler/status/{job_id}"
        )
        response.raise_for_status()
        return response.json()
```

---

## âš™ï¸ é«˜çº§é…ç½® | Advanced Configuration

### 1. ä»£ç†è®¾ç½® | Proxy Configuration

```java
Site site = Site.me()
    .setHttpProxy(new HttpHost("proxy.example.com", 8080));
```

### 2. ç”¨æˆ·ä»£ç†è½®æ¢ | User-Agent Rotation

```java
Site site = Site.me()
    .setUserAgent("Mozilla/5.0 (Windows NT 10.0; Win64; x64)...")
    .addHeader("Accept", "text/html,application/xhtml+xml")
    .addHeader("Accept-Language", "zh-CN,zh;q=0.9,en;q=0.8");
```

### 3. JavaScript æ¸²æŸ“ | JavaScript Rendering

å¯¹äºéœ€è¦ JavaScript æ¸²æŸ“çš„é¡µé¢ï¼Œä½¿ç”¨ Selenium æ‰©å±•ï¼š

```xml
<dependency>
    <groupId>us.codecraft</groupId>
    <artifactId>webmagic-selenium</artifactId>
    <version>0.9.1</version>
</dependency>
```

```java
Spider.create(new MyPageProcessor())
    .setDownloader(new SeleniumDownloader())
    .addUrl("https://example.com")
    .run();
```

### 4. åˆ†å¸ƒå¼çˆ¬è™« | Distributed Crawling

ä½¿ç”¨ Redis å®ç°åˆ†å¸ƒå¼é˜Ÿåˆ—ï¼š

```xml
<dependency>
    <groupId>us.codecraft</groupId>
    <artifactId>webmagic-extension</artifactId>
    <version>0.9.1</version>
</dependency>
```

```java
Spider.create(new MyPageProcessor())
    .setScheduler(new RedisScheduler("localhost"))
    .addUrl("https://example.com")
    .thread(10)
    .run();
```

---

## ğŸ” æœ€ä½³å®è·µ | Best Practices

### 1. çˆ¬è™«ç¤¼èŠ‚ | Crawler Etiquette

```java
Site site = Site.me()
    .setSleepTime(1000)          // è¯·æ±‚é—´éš” 1 ç§’
    .setRetryTimes(3)             // é‡è¯•æ¬¡æ•°
    .setTimeOut(10000)            // è¶…æ—¶æ—¶é—´ 10 ç§’
    .setCharset("UTF-8")          // å­—ç¬¦ç¼–ç 
    .setUserAgent("SurfSense Bot/1.0"); // æ ‡è¯†çˆ¬è™«
```

### 2. é”™è¯¯å¤„ç† | Error Handling

```java
@Override
public void process(Page page) {
    try {
        // çˆ¬å–é€»è¾‘
        String title = page.getHtml().xpath("//title/text()").toString();
        
        if (title == null || title.isEmpty()) {
            page.setSkip(true); // è·³è¿‡æ— æ•ˆé¡µé¢
            return;
        }
        
        page.putField("title", title);
    } catch (Exception e) {
        logger.error("Error processing page: " + page.getUrl(), e);
        page.setSkip(true);
    }
}
```

### 3. æ•°æ®æ¸…æ´— | Data Cleaning

```java
// æ¸…ç† HTML æ ‡ç­¾
String cleanText = page.getHtml()
    .xpath("//body/allText()")
    .toString()
    .replaceAll("\\s+", " ")  // è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
    .trim();

// æå–ç‰¹å®šæ ¼å¼æ•°æ®
String date = page.getHtml()
    .xpath("//span[@class='date']/text()")
    .toString();
```

### 4. æ€§èƒ½ä¼˜åŒ– | Performance Optimization

- **åˆç†è®¾ç½®çº¿ç¨‹æ•°**: æ ¹æ®ç›®æ ‡ç½‘ç«™è´Ÿè½½èƒ½åŠ›è°ƒæ•´
- **ä½¿ç”¨è¿æ¥æ± **: å¤ç”¨ HTTP è¿æ¥
- **å®ç°å¢é‡çˆ¬å–**: åªçˆ¬å–æ›´æ–°çš„å†…å®¹
- **ä½¿ç”¨å¸ƒéš†è¿‡æ»¤å™¨**: å»é™¤é‡å¤ URL

---

## ğŸ”’ å®‰å…¨è€ƒè™‘ | Security Considerations

### 1. Robots.txt éµå®ˆ | Robots.txt Compliance

```java
// WebMagic doesn't have built-in robots.txt support
// Use crawler-commons library for robots.txt checking
// Add dependency:
// <dependency>
//     <groupId>com.github.crawler-commons</groupId>
//     <artifactId>crawler-commons</artifactId>
//     <version>1.2</version>
// </dependency>

import crawlercommons.robots.SimpleRobotRules;
import crawlercommons.robots.SimpleRobotRulesParser;

public boolean isAllowed(String url) {
    SimpleRobotRulesParser parser = new SimpleRobotRulesParser();
    // Fetch and parse robots.txt
    // Check if URL is allowed for the user-agent
    SimpleRobotRules rules = parser.parseContent(
        url, 
        robotsTxtContent, 
        "text/plain", 
        "SurfSense Bot"
    );
    return rules.isAllowed(url);
}
```

### 2. è®¿é—®é¢‘ç‡é™åˆ¶ | Rate Limiting

```java
Site site = Site.me()
    .setSleepTime(1000)           // æœ€å°é—´éš”
    .setRetrySleepTime(3000);     // é‡è¯•é—´éš”
```

### 3. IP è½®æ¢ | IP Rotation

```java
import org.apache.http.HttpHost;
import java.util.*;
import java.util.concurrent.atomic.AtomicInteger;

// Implement custom proxy rotation
public class ProxyProvider {
    private List<HttpHost> proxies;
    private AtomicInteger counter = new AtomicInteger(0);
    
    public ProxyProvider(List<HttpHost> proxies) {
        this.proxies = proxies;
    }
    
    public HttpHost getNextProxy() {
        int index = counter.getAndIncrement() % proxies.size();
        return proxies.get(index);
    }
}

// Use in Site configuration
ProxyProvider proxyProvider = new ProxyProvider(Arrays.asList(
    new HttpHost("proxy1.example.com", 8080),
    new HttpHost("proxy2.example.com", 8080)
));

Site site = Site.me()
    .setHttpProxy(proxyProvider.getNextProxy());
```

---

## ğŸ“Š ç›‘æ§ä¸æ—¥å¿— | Monitoring and Logging

### 1. çˆ¬å–ç»Ÿè®¡ | Crawl Statistics

```java
import us.codecraft.webmagic.*;
import us.codecraft.webmagic.pipeline.*;
import java.util.concurrent.atomic.AtomicInteger;

Spider spider = Spider.create(new MyPageProcessor())
    .addUrl("https://example.com")
    .addPipeline(new ConsolePipeline())
    .thread(5);

spider.run();

// WebMagic doesn't provide built-in detailed statistics
// Implement custom statistics tracking using Pipeline
class StatisticsPipeline implements Pipeline {
    private AtomicInteger successCount = new AtomicInteger(0);
    private AtomicInteger errorCount = new AtomicInteger(0);
    
    @Override
    public void process(ResultItems resultItems, Task task) {
        if (resultItems.isSkip()) {
            errorCount.incrementAndGet();
        } else {
            successCount.incrementAndGet();
        }
    }
    
    public int getSuccessCount() {
        return successCount.get();
    }
    
    public int getErrorCount() {
        return errorCount.get();
    }
}

// Usage
StatisticsPipeline stats = new StatisticsPipeline();
spider.addPipeline(stats);
spider.run();

System.out.println("æˆåŠŸ: " + stats.getSuccessCount());
System.out.println("å¤±è´¥: " + stats.getErrorCount());
```

### 2. æ—¥å¿—é…ç½® | Logging Configuration

```xml
<!-- logback.xml -->
<configuration>
    <appender name="FILE" class="ch.qos.logback.core.FileAppender">
        <file>logs/webmagic-crawler.log</file>
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>
    
    <logger name="us.codecraft.webmagic" level="INFO"/>
    
    <root level="INFO">
        <appender-ref ref="FILE" />
    </root>
</configuration>
```

---

## ğŸ†˜ æ•…éšœæ’é™¤ | Troubleshooting

### å¸¸è§é—®é¢˜ | Common Issues

#### 1. **è¿æ¥è¶…æ—¶ | Connection Timeout**
```java
Site site = Site.me()
    .setTimeOut(30000)  // å¢åŠ åˆ° 30 ç§’
    .setRetryTimes(5);
```

#### 2. **ç¼–ç é—®é¢˜ | Encoding Issues**
```java
Site site = Site.me()
    .setCharset("UTF-8");  // æˆ– "GBK" for some Chinese sites
```

#### 3. **åçˆ¬è™«æªæ–½ | Anti-Crawler Measures**
- ä½¿ç”¨ä»£ç† IP æ± 
- éšæœºåŒ– User-Agent
- å¢åŠ è¯·æ±‚é—´éš”
- ä½¿ç”¨ Selenium æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨

#### 4. **å†…å­˜æº¢å‡º | Out of Memory**
```java
// Use BloomFilterDuplicateRemover for memory-efficient duplicate detection
Spider.create(new MyPageProcessor())
    .setScheduler(new QueueScheduler()
        .setDuplicateRemover(new BloomFilterDuplicateRemover()))
    .thread(5)
    .run();

// Or adjust JVM heap size
// java -Xmx2g -jar webmagic-crawler.jar
```

---

## ğŸ“š å­¦ä¹ èµ„æº | Learning Resources

### å®˜æ–¹æ–‡æ¡£ | Official Documentation
- [WebMagic å®˜æ–¹æ–‡æ¡£](http://webmagic.io/docs/zh/)
- [WebMagic GitHub](https://github.com/code4craft/webmagic)

### æ•™ç¨‹ä¸ç¤ºä¾‹ | Tutorials and Examples
- [WebMagic in Action](https://github.com/code4craft/webmagic/tree/master/webmagic-samples)
- [WebMagic ä¸­æ–‡æ•™ç¨‹](http://webmagic.io/docs/zh/posts/ch1-overview/)

### ç¤¾åŒºæ”¯æŒ | Community Support
- GitHub Issues: [WebMagic Issues](https://github.com/code4craft/webmagic/issues)
- Gitter èŠå¤©å®¤: [WebMagic Gitter](https://gitter.im/code4craft/webmagic)

---

## ğŸ”„ ä¸ SurfSense åŠŸèƒ½å¯¹æ¯” | Feature Comparison with SurfSense

| åŠŸèƒ½ | SurfSense åŸç”Ÿ | WebMagic é›†æˆ |
|-----|---------------|---------------|
| **URL æŠ“å–** | âœ… | âœ… |
| **Markdown è¾“å‡º** | âœ… | éœ€è¦è½¬æ¢ |
| **JavaScript æ¸²æŸ“** | âœ… (Chromium) | âœ… (Selenium) |
| **å¹¶å‘æ§åˆ¶** | âœ… | âœ… |
| **åˆ†å¸ƒå¼** | âŒ | âœ… |
| **å¢é‡çˆ¬å–** | éƒ¨åˆ† | âœ… |
| **æ·±åº¦çˆ¬å–** | âŒ | âœ… |
| **è‡ªå®šä¹‰è§„åˆ™** | æœ‰é™ | âœ… é«˜åº¦çµæ´» |

---

## ğŸ’¡ ä½¿ç”¨å»ºè®® | Usage Recommendations

### ä½•æ—¶ä½¿ç”¨ WebMagic | When to Use WebMagic

âœ… **æ¨èä½¿ç”¨ WebMagic**:
- éœ€è¦çˆ¬å–å¤§é‡ä¸­æ–‡ç½‘ç«™
- éœ€è¦åˆ†å¸ƒå¼çˆ¬è™«èƒ½åŠ›
- éœ€è¦é«˜åº¦å®šåˆ¶åŒ–çš„çˆ¬å–é€»è¾‘
- å¸Œæœ›é™ä½äº‘æœåŠ¡æˆæœ¬
- æœ‰ Java å¼€å‘èƒ½åŠ›çš„å›¢é˜Ÿ

âš ï¸ **ç»§ç»­ä½¿ç”¨ Firecrawl/Chromium**:
- éœ€è¦ç®€å•å¿«é€Ÿçš„å•é¡µé¢æŠ“å–
- ä¸æƒ³ç»´æŠ¤é¢å¤–çš„ Java æœåŠ¡
- ä¸»è¦å¤„ç†åŠ¨æ€ JavaScript é¡µé¢
- å°è§„æ¨¡æ•°æ®æŠ“å–

### æ··åˆä½¿ç”¨æ–¹æ¡ˆ | Hybrid Approach

æœ€ä½³å®è·µæ˜¯æ ¹æ®åœºæ™¯é€‰æ‹©ï¼š

```python
# Example: Smart crawler selection in SurfSense backend
from app.tasks.document_processors.url_crawler import add_crawled_url_document

async def crawl_url_smart(url: str, session: AsyncSession, search_space_id: int, user_id: str):
    """æ™ºèƒ½é€‰æ‹©çˆ¬è™«æ–¹æ¡ˆ | Smart crawler selection"""
    
    # Helper function to determine if large-scale crawling is needed
    def is_large_scale_task(url: str) -> bool:
        # Example logic: check if URL requires deep crawling
        # This is a placeholder - implement based on your needs
        return "site-to-crawl-deeply.com" in url
    
    # Initialize WebMagic client (configuration needed)
    webmagic_client = WebMagicClient(base_url="http://webmagic-service:8080")
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºå¤§è§„æ¨¡ä»»åŠ¡
    if is_large_scale_task(url):
        # ä½¿ç”¨ WebMagic è¿›è¡Œæ·±åº¦çˆ¬å–
        return await webmagic_client.crawl_url(url, depth=3, threads=10)
    else:
        # ä½¿ç”¨ç°æœ‰çš„ Firecrawl/Chromium
        return await add_crawled_url_document(session, url, search_space_id, user_id)
```

---

## ğŸ“ æ€»ç»“ | Summary

WebMagic æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ Java çˆ¬è™«æ¡†æ¶ï¼Œç‰¹åˆ«é€‚åˆä¸­æ–‡ç½‘ç«™çš„å¤§è§„æ¨¡æ•°æ®æŠ“å–ã€‚é€šè¿‡ç‹¬ç«‹å¾®æœåŠ¡çš„æ–¹å¼ï¼Œå¯ä»¥å¾ˆå¥½åœ°ä¸ SurfSense é›†æˆï¼Œä¸ºç”¨æˆ·æä¾›æ›´çµæ´»ã€æ›´å¼ºå¤§çš„ç½‘ç»œçˆ¬å–èƒ½åŠ›ã€‚

WebMagic is a powerful Java crawler framework, particularly suitable for large-scale data extraction from Chinese websites. By integrating it as a standalone microservice, it can work seamlessly with SurfSense to provide users with more flexible and powerful web crawling capabilities.

---

## ğŸ”— ç›¸å…³é“¾æ¥ | Related Links

- [SurfSense æ–‡æ¡£](https://www.surfsense.net/docs/)
- [WebMagic GitHub](https://github.com/code4craft/webmagic)
- [WebMagic Gitee é•œåƒ](https://gitee.com/flashsword20/webmagic.git)
- [ä¸­æ–‡ LLM é…ç½®æŒ‡å—](./chinese-llm-setup.md)

---

**å¦‚æœ‰é—®é¢˜ï¼Œæ¬¢è¿åœ¨ [SurfSense Discord](https://discord.gg/ejRNvftDp9) è®¨è®ºï¼**

**For questions, feel free to discuss in [SurfSense Discord](https://discord.gg/ejRNvftDp9)!**
